### ğŸ®Gym

Gymå¯ä»¥å»ºç«‹ä¸€ä¸ªLunarLanderç¯å¢ƒï¼Œè¿™ä¸ªç¯å¢ƒæ¨¡æ‹Ÿäº†ç™»æœˆå°è‰‡é™è½åœ¨æœˆçƒè¡¨é¢æ—¶çš„æƒ…å½¢ã€‚

ç¯å¢ƒåŒæ—¶åŒ…æ‹¬äº†agentå’Œenvironment,agentä¸æ–­åœ°å’Œenvironmentäº’åŠ¨ã€‚

`env.observation_space`ï¼šè¿”å›ä¸€ä¸ªå…«ç»´å‘é‡ï¼Œä½œä¸ºobservation/state

`env.action_space`ï¼šè¿”å›Discrete(4)ï¼Œè¯´æ˜agentå¯ä»¥é‡‡å–å››ç§ç¦»æ•£çš„è¡ŒåŠ¨ï¼ˆ0-ä¸é‡‡å–ä»»ä½•è¡ŒåŠ¨ï¼›1-ä¸»å¼•æ“å‘å·¦å–·å°„ï¼›2-ä¸»å¼•æ“å‘ä¸‹å–·å°„ï¼›3-ä¸»å¼•æ“å‘å³å–·å°„ï¼‰

`env.step(action)`:è¿”å›(observation,reward,done,_),è¡¨ç¤ºé‡‡å–äº†actionä»¥å,agentå¯¹äºenvironmentçš„observationï¼Œè¿™ä¸€ä¸ªactionè·å¾—çš„rewardï¼Œå’Œåˆ°è¿™ä¸€æ­¥ä¸ºæ­¢è¿™ä¸ªepisodeæ˜¯å¦ç»“æŸäº†ï¼ˆå½“ç™»æœˆå°è‰‡æˆåŠŸç€é™†ï¼Œæˆ–è€…ä¸å¹¸å æ¯æ—¶ï¼Œä¸€ä¸ªepisodeå°±ç»“æŸäº†ï¼Œdoneä¸ºTrueï¼‰

`reward:`

| case               | reward  |
| ------------------ | ------- |
| å°è‰‡å æ¯           | -100    |
| å°è‰‡åœ¨é»„æ——ä¹‹é—´ç€é™† | 100~140 |
| ä¸»å¼•æ“å‘ä¸‹å–·å°„     | -0.3    |
| å°è‰‡å®Œå…¨é™æ­¢       | 100     |

ä¸€ä¸ªrandom agentçš„ä¾‹å­ï¼š

```python
# ä»»ä½•æ“ä½œä¹‹å‰ï¼Œå…ˆé‡ç½®ç¯å¢ƒ
env.reset()

#å±•ç¤ºå›¾å½¢ç•Œé¢
img = plt.imshow(env.render(mode='rgb_array'))

#é‡‡æ ·ä¸€æ¬¡
done = False
while not done:
    #éšæœºé‡‡å–ä¸€ä¸ªåŠ¨ä½œ
    action = env.action_space.sample()
    #æ›´æ–°ä¸€æ­¥ï¼Œè¿™ä¸€æ­¥agentå’Œenvironmentè¿›è¡Œäº†äº¤äº’
    observation, reward, done, _ = env.step(action)
	#æ›´æ–°å›¾å½¢ç•Œé¢
    img.set_data(env.render(mode='rgb_array'))
    display.display(plt.gcf())
    display.clear_output(wait=True)
```



### ğŸ”®Model

##### step 1. policy network

input:observation,ä¸€ä¸ªå…«ç»´å‘é‡

output:ç¦»æ•£çš„å››ä¸ªåŠ¨ä½œä¹‹ä¸€

ç›®çš„å°±æ˜¯ç”¨ç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆè¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»

##### step 2. policy gradient agent

agentæ­é…policy networkæ¥é‡‡å–è¡ŒåŠ¨

`learn(log_probs, rewards)`:æ ¹æ®è®°å½•çš„log probabilitieså’Œrewardsæ¥æ›´æ–°policy network

`sample(state)`:æ ¹æ®å½“å‰enviromentçš„state,åˆ©ç”¨policy networkå¾—å‡ºåº”è¯¥é‡‡å–çš„è¡ŒåŠ¨ã€‚æŒ‰ç…§ä¼ å…¥çš„probsä¸­ç»™å®šçš„æ¦‚ç‡ï¼Œåœ¨ç›¸åº”çš„ä½ç½®å¤„è¿›è¡Œå–æ ·ï¼Œå–æ ·è¿”å›çš„æ˜¯è¯¥ä½ç½®çš„æ•´æ•°ç´¢å¼•ã€‚å›ä¼ æŠ½æ ·å¾—åˆ°çš„actionå’Œæ­¤æ¬¡æŠ½æ ·çš„log probabilitiesã€‚

### ğŸš†Train

change mode to enable optimizer to update parameters

`model.train()`

```python
for each episode:
	#reset state
	state = env.reset()
    total_reward,total_step=0,0
    while True:
        # sample action based on current state
        action, log_prob = agent.sample(state)
        # a step forward
        next_state, reward, done, _ = env.step(action)
        # make records
        log_probs.append(log_prob) # add log_probs
        total_reward += reward# add reward to the episode's total reward
        total_step += 1#count steps
        state = next_state# update state
        if done:
            final_rewards.append(reward)
            total_rewards.append(total_reward)
            # set rewards in one episode the same
            rewards.append(np.full(total_step, total_reward))
            break
    rewards = np.concatenate(rewards, axis=0)
    #normalization
    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)# avoid divided by 0
    # calculate loss and update parameters by the agent based on log probabilities and 
    # rewards for each episode
    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))
```



- np.concatenate((a1,a2,...),axis=0)

  -åŠŸèƒ½ï¼šæ•°ç»„æ‹¼æ¥

  -(a1,a2,...):éœ€è¦è¢«è¿æ¥çš„æ•°ç»„ï¼Œé™¤äº†axiså¯¹åº”çš„è½´ï¼Œæ•°ç»„å…¶ä»–ç»´åº¦å¿…é¡»ä¸€æ ·

  -axis:è¿æ¥æ–¹å‘

  -e.g.

  ```python
  a = np.array([[1, 2], [3, 4]])#(2*2)
  b = np.array([[5, 6]])#(1*2)
  np.concatenate((a, b), axis=0)# 2*2,1*2
  array([[1, 2],
         [3, 4],
         [5, 6]])#3*2
  
  np.concatenate((a, b.T), axis=1)#2*2,2*1
  array([[1, 2, 5],
         [3, 4, 6]])#2*3
  
  np.concatenate((a, b), axis=None)
  array([1, 2, 3, 4, 5, 6])
  ```

  

- torch.stack((a1,a2,...),dim=0)

  -åŠŸèƒ½ï¼š æ²¿ä¸€ä¸ªæ–°ç»´åº¦å¯¹è¾“å…¥å¼ é‡åºåˆ—è¿›è¡Œè¿æ¥ï¼Œåºåˆ—ä¸­æ‰€æœ‰å¼ é‡åº”ä¸ºç›¸åŒå½¢çŠ¶ï¼›stack å‡½æ•°è¿”å›çš„ç»“æœä¼šæ–°å¢ä¸€ä¸ªç»´åº¦ï¼Œè€Œstackï¼ˆï¼‰å‡½æ•°æŒ‡å®šçš„dimå‚æ•°ï¼Œå°±æ˜¯æ–°å¢ç»´åº¦çš„ï¼ˆä¸‹æ ‡ï¼‰ä½ç½®

  -(a1,a2,...):éœ€è¦è¢«è¿æ¥çš„tensor

  -dim:æ‹¼æ¥ç»´åº¦

  -e.g.

  ```python
  """
  torch.stackçš„å‚æ•°dimçš„åŠŸèƒ½æ˜¯æ ‡è¯†å‡ºï¼Œæ–°å¢çš„é‚£ä¸€ç»´ä½ç½®ï¼Œå¦‚æœdim=0ï¼Œåˆ™æ–°å¢çš„ç»´åº¦ä¸ºç¬¬ä¸€ç»´ï¼Œç¬¬ä¸€ç»´å°±æ˜¯åŸæ¥çš„tensoræ‹¼æ¥è€Œæˆã€‚è€Œç¬¬äºŒç»´ã€ç¬¬ä¸‰ç»´åˆ™æ˜¯åŸæ¥ä¸‰ä¸ªtensorçš„ç»§æ‰¿ã€‚ä»¥æ¬¡ç±»æ¨dimä¸º1å’Œ2çš„æƒ…å†µã€‚
  """
  import torch
  a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
  b = torch.tensor([[11, 22, 33], [44, 55, 66], [77, 88, 99]])
  c = torch.stack([a, b], dim=0)
  print(c)
  tensor([[[ 1,  2,  3],
           [ 4,  5,  6],
           [ 7,  8,  9]],
   
          [[11, 22, 33],
           [44, 55, 66],
           [77, 88, 99]]])
  
  
  c = torch.stack([a, b], dim=1)
  print(c)
  tensor([[[ 1,  2,  3],
           [11, 22, 33]],
   
          [[ 4,  5,  6],
           [44, 55, 66]],
   
          [[ 7,  8,  9],
           [77, 88, 99]]])
  
  c = torch.stack([a, b], dim=2)
  print(c)
  tensor([[[ 1, 11],
           [ 2, 22],
           [ 3, 33]],
   
          [[ 4, 44],
           [ 5, 55],
           [ 6, 66]],
   
          [[ 7, 77],
           [ 8, 88],
           [ 9, 99]]])
  ```

  

### ğŸ”¨Test

change mode to fix parameters

`model.eval()`

```python
agent.network.eval()
# ä»»ä½•æ“ä½œä¹‹å‰ï¼Œå…ˆé‡ç½®ç¯å¢ƒ
state=env.reset()
#å±•ç¤ºå›¾å½¢ç•Œé¢
img = plt.imshow(env.render(mode='rgb_array'))
total_reward=0
done=False
while not done:
    # è®©agenté€‰å–ä¸€ä¸ªåŠ¨ä½œ
    action,_=agent.sample(state)
    #å‰è¿›ä¸€æ­¥
    new_state,reward,done,_=env.step(action)
    #æ›´æ–°å½“å‰çŠ¶æ€ï¼Œæ€»çš„å›æŠ¥
    state=new_state
    total_reward+=reward
    #æ›´æ–°å›¾å½¢ç•Œé¢
    img.set_data(env.render(mode='rgb_array'))
    display.display(plt.gcf())
    display.clear_output(wait=True)
```

