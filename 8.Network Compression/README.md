Model Compression æœ‰å¾ˆå¤šç§é—¨æ´¾ï¼š

- Knowledge Distillation: è®©å° model èƒå–å¤§ model çš„çŸ¥è¯†
- Network Pruning: å°†å·²ç»å­¦ä¹ å¥½çš„çš„å¤§ model åšå‰ªæï¼ˆweight/neuronï¼‰,è®©æ•´ä¸ªæ¨¡å‹å˜å°
- Weight Quantization: ç”¨æ›´å¥½çš„æ–¹å¼æ¥è¡¨ç° model ä¸­çš„å‚æ•°ï¼Œä»¥æ­¤é™ä½è¿ç®—æ¶ˆè€—/å®¹é‡ã€‚
- Design Architecture: å°†åŸå§‹çš„ layer ç”¨æ›´å°çš„å‚æ•°æ¥è¡¨ç°ã€‚ (ä¾‹å¦‚ Convolution ï¿« Depthwise & Pointwise Convolution)

### Design Architecture

##### æ ¸å¿ƒ

ä½¿ç”¨Depthwise & Pointwise Convolution Layeæ¥å modelï¼Œæ€»å…±æœ‰7å±‚CNNã€‚

- DW(Depthwise Convolution Layer)ï¼šä¸€å¼ feature mapå„è‡ªç”¨ä¸€ä¸ªfilterå¤„ç†
- PW(Pointwise Convolution Layer)ï¼šæŠŠæ‰€æœ‰feature mapçš„å•ä¸ªpixelèµ„è®¯åˆåœ¨ä¸€èµ·(å°±æ˜¯1ä¸ªpixelçš„Fully Connected Layer)ã€‚

å°†åŸæœ¬çš„Convolution Layeræ¢æˆDw & Pwå¾Œï¼ŒAccuracyé€šå¸¸ä¸ä¼šé™å¾ˆå¤šã€‚åŒæ—¶ï¼Œæ¨¡å‹çš„å‚æ•°æ˜¾è‘—å‡å°‘äº†ã€‚

##### å®ç°

```python
# ä¸€èˆ¬çš„Convolution, weightå¤§å° = in_chs * out_chs * kernel_size^2
nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding)

# Group Convolution, Groupæ•°ç›®å¯ä»¥è‡ªè¡Œæ§åˆ¶ï¼Œè¡¨ç¤ºè¦åˆ†æˆå‡ ç¾¤ã€‚å…¶ä¸­in_chså’Œout_chså¿…é¡»è¦å¯ä»¥è¢«groupsæ•´é™¤ã€‚(ä¸ç„¶æ²’åŠæ³•åˆ†ç¾¤ã€‚)
nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups)

# Depthwise Convolution, è¼¸å…¥chs=è¾“å‡ºchs=Groupsæ•°ç›®, weightå¤§å° = in_chs * kernel_size^2
nn.Conv2d(in_chs, out_chs=in_chs, kernel_size, stride, padding, groups=in_chs)

# Pointwise Convolution, ä¹Ÿå°±æ˜¯1 by 1 convolution, weightå¤§å° = in_chs * out_chs
nn.Conv2d(in_chs, out_chs, 1)
```



### Knowledge Distillation

##### What?

åˆ©ç”¨å¤§modelé¢„æµ‹çš„logitsç»™å°modelå½“ä½œæ ‡å‡†ï¼Œä»è€Œå‘Šè¯‰å°çš„modelå¦‚ä½•å­¦ä¹ .

##### Why?

- å½“dataä¸æ˜¯å¾ˆå¹²å‡€çš„æ—¶å€™ï¼Œå¯¹ä¸€èˆ¬modelæ¥è¯´æ˜¯noiseï¼Œä¼šå¹²æ‰°å­¦ä¹ ã€‚é€šè¿‡å»å­¦ä¹ å…¶ä»–å¤§çš„modelé¢„æµ‹çš„logitsä¼šæ¯”è¾ƒå¥½ã€‚
- labelå’Œlabelä¹‹é—´å¯èƒ½å­˜åœ¨å…³è”ï¼Œè¿™å¯ä»¥å¼•å¯¼å°çš„modelå»å­¦ä¹ ã€‚ï¼ˆå¦‚æ•°å­—8å’Œ6ï¼Œ9ï¼Œ0å°±æœ‰å…³ç³»ï¼‰
- å¼±åŒ–å·²ç»å­¦ä¹ ä¸é”™çš„targetï¼Œé¿å…è®©å…¶gradientå¹²æ‰°å…¶ä»–æ²¡æœ‰å­¦ä¹ å¥½çš„taskã€‚

##### How?

 $Loss = \alpha T^2 \times KL(\frac{\text{Teacher's Logits}}{T} || \frac{\text{Student's Logits}}{T}) + (1-\alpha)(\text{Original Loss})$

æ”¹ä¸€ä¸‹æŸå¤±å‡½æ•°ï¼Œåˆ†æˆhard_losså’Œsoft_lossä¸¤éƒ¨åˆ†ã€‚hard_lossé’ˆå¯¹hard_labels,soft_lossé’ˆå¯¹teacher_outputsã€‚

```python
def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):
    # ä¸€èˆ¬çš„cross entropy
    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)
    # å°†logitsçš„log_softmaxå¯¹ç›®æ ‡æ¦‚ç‡(teacherçš„logits/Tåsoftmax)åšKL Divergence
    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs / T, dim=1),
                                                    F.softmax(teacher_outputs / T, dim=1)) * (alpha * T * T)
    return hard_loss + soft_loss
```

##### Steps

1. å°è£…dataset
2. å¾—åˆ°ç›¸åº”çš„dataloader
3. è½½å…¥teacher_net:torchvisionæä¾›çš„ResNet18ï¼ŒæŠŠnum_classesæ”¹æˆ11åloadè¿›å»å³å¯
4. å®šä¹‰optimizerï¼Œlossï¼Œå¼€å§‹è®­ç»ƒ

### Network Pruning

##### What?

å¯¹å·²ç»å­¦ä¹ å®Œçš„modelä¸­çš„Neuronè¿›è¡Œåˆ å‡ï¼Œè®©ç½‘ç»œæ›´ç˜¦ã€‚Neuron Pruningéœ€è¦è¡¡é‡Neuronçš„é‡è¦æ€§ï¼Œå°†ä¸é‡è¦çš„Neuronåˆ å‡æ‰ã€‚

è¡¡é‡æ–¹æ³•ï¼šé€šè¿‡batchnorm layerçš„ğ›¾å› å­ä¾†æ±ºå®šneuronçš„é‡è¦æ€§ã€‚ (by paper - Network Slimming)

##### How?

- å°†StudentNet(width_mult=Î±)çš„neuronç»è¿‡ç­›é€‰åç§»æ¤åˆ°StudentNet(width_mult=Î²)ã€‚(Î±>Î²)
- ç­›é€‰åªéœ€è¦æŠ“å‡ºæ¯ä¸€ä¸ªblockçš„batchnormçš„Î³å³å¯ã€‚

Modelçš„ä¸­é—´ä¸¤å±‚æ¯”è¾ƒç‰¹æ®Š:

| Layer                           | Output of Channels |
| ------------------------------- | ------------------ |
| Input                           | in_chs             |
| Depthwise(in_chs)               | in_chs             |
| BatchNorm(in_chs)               | in_chs             |
| Pointwise(in_chs, **mid_chs**)  | **mid_chs**        |
| **Depthwise(mid_chs)**          | **mid_chs**        |
| **BatchNorm(mid_chs)**          | **mid_chs**        |
| Pointwise(**mid_chs**, out_chs) | out_chs            |

åˆ©ç”¨ç¬¬äºŒä¸ªBatchNormæ¥åšç­›é€‰çš„æ™‚å€™ï¼Œè·Ÿä»–çš„Neuronæœ‰ç›´æ¥å…³ç³»çš„æ˜¯è¯¥å±‚çš„Depthwise&Pointwiseä»¥åŠä¸Šå±‚çš„Pointwiseã€‚
å› æ­¤å†åšneuronç­›é€‰æ—¶è®°å¾—è¦å°†è¿™å››ä¸ª(åŒ…æ‹¬è‡ªå·±, bn)ä¹Ÿè¦åŒæ—¶pruneæ‰ã€‚

Modelå…¶ä»–çš„å±‚ï¼š

|      | name      | meaning                     | code                              | weight shape |
| ---- | --------- | --------------------------- | --------------------------------- | ------------ |
| 0    | cnn.{i}.0 | Depthwise Convolution Layer | nn.Conv2d(x, x, 3, 1, 1, group=x) | (x, 1, 3, 3) |
| 1    | cnn.{i}.1 | Batch Normalization         | nn.BatchNorm2d(x)                 | (x)          |
| 2    |           | ReLU6                       | nn.ReLU6                          |              |
| 3    | cnn.{i}.3 | Pointwise Convolution Layer | nn.Conv2d(x, y, 1),               | (y, x, 1, 1) |
| 4    |           | MaxPooling                  | nn.MaxPool2d(2, 2, 0)             |              |

æˆ‘ä»¬è¦æŠ“å–çš„$\gamma$ç³»æ•°åœ¨åœ¨cnn.{i}.1.weightå†…ã€‚

### Weight Quantization

æ ¸å¿ƒæ€æƒ³ï¼šuse less bit to represent a value.åœ¨å®ä½œä¸­ï¼Œå°†32-bitçš„tensorè½¬æ¢æˆæ›´å°‘bitçš„ndarrayå­˜èµ·æ¥ã€‚

å®šä¹‰`encode`å’Œ`decode`å‡½æ•°ï¼Œå‰è€…å°†æ¨¡å‹å‚æ•°å‹ç¼©æˆä½bitä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œåè€…å°†æ–‡ä»¶ä¸­ä¿å­˜çš„å‚æ•°è¿˜åŸæˆtorch.tensorï¼Œå†å­˜å…¥state_dictä¸­ã€‚

- to 16 bitï¼šç”¨`np.float16`è¿›è¡Œè½¬åŒ–å‹ç¼©å³å¯

- to 8 bitï¼šå› ä¸ºæ²’æœ‰8-bitçš„floatï¼Œæ‰€ä»¥æˆ‘ä»¬å…ˆå¯¹æ¯ä¸ªweightè®°å½•æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œè¿›è¡Œmin-maxæ­£åˆ™åŒ–åä¹˜ä¸Š255å†å››èˆäº”å…¥ï¼Œå°±å¯ä»¥ç”¨np.uint8å­˜å–äº†ã€‚

  è½¬åŒ–å…¬å¼ï¼š$W' = round(\frac{W - \min(W)}{\max(W) - \min(W)} \times (2^8 - 1)$)