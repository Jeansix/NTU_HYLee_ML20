### ğŸ“•Data

##### Data Form

NumPy ä¸º ndarray å¯¹è±¡å¼•å…¥äº†ä¸€ä¸ªç®€å•çš„æ–‡ä»¶æ ¼å¼ï¼š.npy

npy æ–‡ä»¶ç”¨äºå­˜å‚¨é‡å»º ndarray æ‰€éœ€çš„æ•°æ®ã€å›¾å½¢ã€dtype å’Œå…¶ä»–ä¿¡æ¯ã€‚

trainX.npy

valX.npy

valY.npy

##### Data Processing

å½’ä¸€åŒ–ï¼›ç»´åº¦ä¹‹é—´æ¢é¡ºåº

##### Dataset encapsulation

åˆ©ç”¨Datasetç±»å°è£…æˆImageDataset

### ğŸ”®Model

##### Auto encoder for CNN

- Encoderï¼šConvolution->pooling->Convolution->pooling->Convolution->pooling->code
- Decoderï¼šcode->deconvolution->unpooling->deconvolution->unpooling->deconvolution->unpooling

deconvolutionçš„æœ¬è´¨å°±æ˜¯convolutionï¼Œnn.ConvTranspose2då°è£…äº†æ­¤åå·ç§¯æ“ä½œ

#### ğŸš†Train

- define criterion

- define optimizer

- change mode to enable optimizer to update parameters

  `model.train()`

- start training!

  ```python
  for each epoch:
      set train_loss to 0
  	load data to gpu
  	optimizer.zero_grad()
  	# generate outputs
  	outputs=model(inputs)
  	# calculate loss
  	loss=criterion(outputs,labels)
  	loss.backward()# calculate gradient
  	optimizer.step() # update parameters
  	update loss
  	checkpoint
  ```

  

### ğŸ”¨Test

```python
# test=inference+predict
load model parameters
model.eval()
inference:ç»è¿‡encoderå¾—åˆ°vector
predict:å¯¹inferenceåçš„ç»“æœè¿›è¡Œclustering,ç„¶ååˆ†ç±»
   Dimension Reduction:Latents Shape: (data_size, 4096)
			   First Reduction Shape: (data_size, 200)
			   Second Reduction Shape: (data_size, 2)
```



### ğŸ–ŠAnalysis

Problem 1(ä½œå›¾) é™åˆ°ä¸¤ç»´åï¼Œä¸åŒç±»ç‚¹ä¹‹é—´æœ‰ä¸€å®šçš„è·ç¦»ã€‚è¯æ˜äº†äºŒç»´çš„æƒ…å†µä¸‹ï¼Œdeep auto encoderçš„ç‚¹æ˜¯åˆ†å¼€çš„ï¼Œè€ŒPCAçš„ç‚¹æ··åœ¨ä¸€èµ·ã€‚

Problem 2 é€‰å–ä¸€äº›å›¾ç‰‡ï¼Œç”»å‡ºåŸå›¾å’Œreconstructä¹‹åçš„å›¾ç‰‡

Problem 3 æŒ‘é€‰10ä¸ªcheckpointçš„model

éšç€epochçš„å¢åŠ ï¼Œreconstruction errorå‡å°ï¼Œaccuracyä¸Šå‡